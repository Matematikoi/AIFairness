{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Intersectional Fairness\n",
    "\n",
    "Using the COMPAS dataset to investigate the impact of protected attributes like race and sex with and without age constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_compas\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "compas_data = load_preproc_data_compas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Race as protected attribute\n",
    "\n",
    "We fix the bias using the reweighing preprocessing technique and then measure the bias assuming sex is the protected attribute.\n",
    "\n",
    "First let's download the dataset and partition it into training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "compas_data = load_preproc_data_compas()\n",
    "dataset_orig_train_race, dataset_orig_test_race = compas_data.split([0.7], shuffle=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set 'race' as the protected attribute and perform the RW technique on the train dataset. It's important to remove sex from the protected attributes, because the default protected attributes for the dataset are both 'race' and 'sex'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race']\n"
     ]
    }
   ],
   "source": [
    "privileged_groups_race = [{'race': 1}]\n",
    "unprivileged_groups_race = [{'race': 0}]\n",
    "\n",
    "rw_race = Reweighing(unprivileged_groups=unprivileged_groups_race, privileged_groups=privileged_groups_race)\n",
    "dataset_transf_train_race = rw_race.fit_transform(dataset_orig_train_race)\n",
    "\n",
    "dataset_transf_train_race.protected_attribute_names.remove('sex')\n",
    "print(dataset_transf_train_race.protected_attribute_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid unnecessary code we don't perform the bias validation, but at this point the instance weights are modified to correct the bias considering only 'race'. Let's now train a logistic regression on the reweighed version of the dataset.\n",
    "\n",
    "Additionally, let's use the model to perform predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_race = LogisticRegression(solver='lbfgs', max_iter=1000, C=1.0, penalty='l2', random_state=0)\n",
    "clf_race.fit(dataset_transf_train_race.features, dataset_transf_train_race.labels.flatten())\n",
    "\n",
    "dataset_bias_test_pred_race = clf_race.predict(dataset_orig_test_race.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put this prediction scores into context let's create a dataframe containing both the original label and the predicted scores using the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bias_test_sex = dataset_orig_test_race.copy()\n",
    "dataset_bias_test_sex.scores = dataset_bias_test_pred_race\n",
    "dataset_bias_test_sex.labels = dataset_orig_test_race.labels\n",
    "\n",
    "test_df_sex = dataset_bias_test_sex.convert_to_dataframe()[0]\n",
    "test_df_sex['model_not_recid'] = dataset_bias_test_sex.scores.flatten()\n",
    "test_df_sex['observed_not_recid'] = 1 - test_df_sex['two_year_recid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in order to measure the bias but considering the model's prediction and 'sex' as the protected attribute, we need to create a standard dataset based on the dataframe. Note that the label is set to the model's prediction and not the original label value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Difference in mean outcomes between unprivileged and privileged groups (sex) = -0.228834\n"
     ]
    }
   ],
   "source": [
    "dataset_sex = StandardDataset(test_df_sex, label_name='model_not_recid', favorable_classes=[0],\n",
    "                 protected_attribute_names=['sex'],\n",
    "                 privileged_classes=[[1]],\n",
    "                 instance_weights_name=None)\n",
    "\n",
    "metric_test_sex = BinaryLabelDatasetMetric(dataset_sex,\n",
    "                                           unprivileged_groups=[{'sex': 0}],\n",
    "                                           privileged_groups=[{'sex': 1}])\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups (sex) = %f\" % metric_test_sex.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clearly here that the model trained on the RW process performed for the 'race' attribute changes a lot the outcomes considering 'sex'. The model creates a bigger bias for the attribute that wasn't considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Sex as protected attribute\n",
    "\n",
    "Let's now follow the same logic but focusing first on 'sex' as the protected attribute and then analyzing the bias on 'race':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex']\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups (race) = -0.276763\n"
     ]
    }
   ],
   "source": [
    "compas_data = load_preproc_data_compas()\n",
    "dataset_orig_train_sex, dataset_orig_test_sex = compas_data.split([0.7], shuffle=True, seed=0)\n",
    "\n",
    "privileged_groups_sex = [{'sex': 1}]\n",
    "unprivileged_groups_sex = [{'sex': 0}]\n",
    "\n",
    "rw_sex = Reweighing(unprivileged_groups=unprivileged_groups_sex, privileged_groups=privileged_groups_sex)\n",
    "dataset_transf_train_sex = rw_sex.fit_transform(dataset_orig_train_sex)\n",
    "\n",
    "dataset_transf_train_sex.protected_attribute_names.remove('race')\n",
    "print(dataset_transf_train_sex.protected_attribute_names)\n",
    "\n",
    "clf_sex = LogisticRegression(solver='lbfgs', max_iter=1000, C=1.0, penalty='l2', random_state=0)\n",
    "clf_sex.fit(dataset_transf_train_sex.features, dataset_transf_train_sex.labels.flatten())\n",
    "\n",
    "dataset_bias_test_pred_sex = clf_sex.predict(dataset_orig_test_sex.features)\n",
    "\n",
    "dataset_bias_test_race = dataset_orig_test_sex.copy()\n",
    "dataset_bias_test_race.scores = dataset_bias_test_pred_sex\n",
    "dataset_bias_test_race.labels = dataset_orig_test_sex.labels\n",
    "\n",
    "test_df_race = dataset_bias_test_race.convert_to_dataframe()[0]\n",
    "test_df_race['model_not_recid'] = dataset_bias_test_race.scores.flatten()\n",
    "test_df_race['observed_not_recid'] = 1 - test_df_race['two_year_recid']\n",
    "\n",
    "dataset_race = StandardDataset(test_df_race, label_name='model_not_recid', favorable_classes=[0],\n",
    "                 protected_attribute_names=['race'],\n",
    "                 privileged_classes=[[1]],\n",
    "                 instance_weights_name=None)\n",
    "\n",
    "metric_test_race = BinaryLabelDatasetMetric(dataset_race,\n",
    "                                           unprivileged_groups=[{'race': 0}],\n",
    "                                           privileged_groups=[{'race': 1}])\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups (race) = %f\" % metric_test_race.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This other model has a different impact, which is normal considered that the two models are trained on different conditions. However the same analysis can be made of this result: the model creates a bigger bias because it disregards 'race' when being built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Age parameter\n",
    "\n",
    "We perform the same analysis with a truncated version of the dataset, including only entries with people of age < 25\n",
    "\n",
    "First considering the initial case, training on 'race' and measuring on 'sex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Difference in mean outcomes between unprivileged and privileged groups (sex) = -0.421062\n"
     ]
    }
   ],
   "source": [
    "test_df_sex_age = test_df_sex[test_df_sex['age_cat=Less than 25'] == 1]\n",
    "\n",
    "dataset_sex_age = StandardDataset(test_df_sex_age, label_name='model_not_recid', favorable_classes=[0],\n",
    "                 protected_attribute_names=['sex'],\n",
    "                 privileged_classes=[[1]],\n",
    "                 instance_weights_name=None)\n",
    "\n",
    "metric_test_sex_age = BinaryLabelDatasetMetric(dataset_sex_age,\n",
    "                                           unprivileged_groups=[{'sex': 0}],\n",
    "                                           privileged_groups=[{'sex': 1}])\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups (sex) = %f\" % metric_test_sex_age.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the effect of this model intensifies and the bias grows even bigger when considering only groups of records with age <25. It's important to understand additionally that there are less records, and therefore bias impacts are multiplied.\n",
    "\n",
    "Now let's see if we have the same behavior with the other case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Difference in mean outcomes between unprivileged and privileged groups (race) = -0.283674\n"
     ]
    }
   ],
   "source": [
    "test_df_race_age = test_df_race[test_df_race['age_cat=Less than 25'] == 1]\n",
    "\n",
    "dataset_race_age = StandardDataset(test_df_race_age, label_name='model_not_recid', favorable_classes=[0],\n",
    "                 protected_attribute_names=['race'],\n",
    "                 privileged_classes=[[1]],\n",
    "                 instance_weights_name=None)\n",
    "\n",
    "metric_test_race_age = BinaryLabelDatasetMetric(dataset_race_age,\n",
    "                                           unprivileged_groups=[{'race': 0}],\n",
    "                                           privileged_groups=[{'race': 1}])\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups (race) = %f\" % metric_test_race_age.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second case the impact is not as big. This means that, regardless of the age, the predictions made by a model trained on 'sex' are similar in terms of 'race'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aif360)",
   "language": "python",
   "name": "aif360"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
